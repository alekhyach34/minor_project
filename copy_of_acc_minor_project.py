# -*- coding: utf-8 -*-
"""Copy of acc Minor Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FniFwoojjx6IABg41AdTWIc28qc1Wika
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer #handles missing data
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score
from sklearn.utils import resample

# Load dataset
data = pd.read_csv("/content/fraud_detection_dataset (1).csv")

# Data Cleaning
data_cleaned = data.drop_duplicates().dropna()

# Splitting data into features and target variable
X = data_cleaned.drop(columns=['Fraudulent'])
y = data_cleaned['Fraudulent']

# Identify categorical columns
categorical_cols = X.select_dtypes(include=['object']).columns

# One-hot encode categorical columns
X_categorical = pd.get_dummies(X[categorical_cols]) #coverts categorical values in dummay or indicater values

# Concatenate one-hot encoded columns with original numeric columns
X_processed = pd.concat([X.drop(columns=categorical_cols), X_categorical], axis=1)

# Imputation
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X_processed)

# Normalization(min max scalling main-0 max-1)/Standardization
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed)

# Balancing Classes
X_balanced, y_balanced = resample(X_scaled, y, replace=True, n_samples=X_scaled.shape[0])

# Splitting into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)

#Data Information
data.head(10)

print("Shape of the datacolumn: ",data.shape)
data.isna().sum()

data.info()

data.describe()

data.dtypes

data.notnull().sum()

data['Fraudulent'].value_counts()

data.shape

data.size

data.duplicated()

data.dropna()

data.columns

data.describe(include=object)

# Get the distinct values of each column
for column in data.columns:
    unique_val=data[column].unique()
    print("unique values of {} are {}".format(column,unique_val))

#Filtering rows based on the presence of null values using the dropna() method:
filtered_dropna = data.dropna(axis=0) # default axis value is zero ==> rows with missing values are deleted
print("Total records Before:" , data.shape)
print("Total records After:" , filtered_dropna.shape)

#Filtering rows based on the presence of null values using the dropna() method:

filtered_dropna_2 = data.dropna(axis=1) # ==> columns with missing values are deleted

print("Total records Before:" , data.shape)
print("Total records After:" , filtered_dropna_2.shape)

dup_test = data[data.duplicated()]
dup_test.shape

# Filter rows based on condition'Transaction_Amount' >= 800
filtered_Transaction_Amount = data[data['Transaction_Amount'] >=800 ]
print("Total records:", len(filtered_Transaction_Amount))
filtered_Transaction_Amount.head()

# Filter rows where 'Customer_Name' startswith 'J'
name_inc=data[data["Customer_Name"].str.startswith("J")]
name_inc

#all columns
cols3=data.loc[0:6, :]
cols3

# Filering specific columns

subset_data = data[['Transaction_ID', 'Customer_Name', 'Transaction_Amount', 'Transaction_Type']]
subset_data

# Accessing specific rows and columns using iloc
# Select the first three rows and the first two columns
rows_and_cols=data.iloc[0:3,0:2]
rows_and_cols

#unique Card_CVV
data["Card_CVV"]

unique_Card_CVV=data[~data["Card_CVV"].duplicated()].loc[:,["Card_CVV"]]

unique_Card_CVV

#top 10 Transaction_Amount values
temp=data[data["Transaction_Amount"].rank(ascending=False)<=10].loc[:,'Transaction_ID':'Transaction_Date']
temp

filterd_num=data.select_dtypes(include='int').iloc[0:5,:]
filterd_num

filterd_num=data.select_dtypes(include='number').iloc[0:5,:]
filterd_num

filterd_obj=data.select_dtypes(include='object').iloc[0:4,:]
filterd_obj

filterd_obj=data.select_dtypes(include='float').iloc[0:4,:]
filterd_obj

# Create new variable : Transaction using Transaction_Amount
data["Transaction"]=-round(data['Transaction_Amount']+25,0)
data.head()

#now delete Nincolumn
data.drop('Transaction', axis=1,inplace=True)

dataa=data.dropna(axis=0)
print(data.shape)

"""**EDA-Exploratory data analysis**"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt


# Display the first few rows of the dataset
print("First few rows of the dataset:")
print(data.head())

# Summary statistics
print("\nSummary statistics of numerical columns:")
print(data.describe())

# Check for missing values
print("\nMissing values:")
print(data.isnull().sum())

# Check the data types of columns
print("\nData types:")
print(data.dtypes)

# Distribution of numerical variables
numerical_cols = data.select_dtypes(include=[np.number]).columns
for col in numerical_cols:
    plt.figure(figsize=(8, 6))
    sns.histplot(data[col], kde=True)
    plt.title(f"Distribution of {col}")
    plt.xlabel(col)
    plt.ylabel("Frequency")
    plt.show()

# Correlation heatmap
try:
    numerical_data = data.select_dtypes(include=[np.number])
    plt.figure(figsize=(10, 8))
    sns.heatmap(numerical_data.corr(), annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
    plt.title("Correlation Heatmap")
    plt.show()
except ValueError as e:
    print("Correlation heatmap cannot be generated due to non-numeric data in the dataset.")

# Boxplot for categorical vs. numerical variables
categorical_cols = data.select_dtypes(include=['object']).columns
for cat_col in categorical_cols:
    for num_col in numerical_cols:
        plt.figure(figsize=(8, 6))
        sns.boxplot(x=cat_col, y=num_col, data=data)
        plt.title(f"{cat_col} vs. {num_col}")
        plt.xlabel(cat_col)
        plt.ylabel(num_col)
        plt.show()

sns.countplot(x='Fraudulent', data=data)

#random forest
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.utils import resample

# Data Cleaning (optional)
data_cleaned = data.drop_duplicates().dropna()

# Splitting data into features and target variable
X = data_cleaned.drop(columns=['Fraudulent'])
y = data_cleaned['Fraudulent']

# Identify categorical columns
categorical_cols = X.select_dtypes(include=['object']).columns

# One-hot encode categorical columns
X_categorical = pd.get_dummies(X[categorical_cols])

# Concatenate one-hot encoded columns with original numeric columns
X_processed = pd.concat([X.drop(columns=categorical_cols), X_categorical], axis=1)

# Imputation
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X_processed)

# Normalization/Standardization
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed)

# Balancing Classes
X_balanced, y_balanced = resample(X_scaled, y, replace=True, n_samples=X_scaled.shape[0])

# Splitting into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)

# Model Training
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Model Evaluation
y_pred = model.predict(X_test)

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(conf_matrix)

# Accuracy Score
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
# Plot Confusion Matrix as Heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')
plt.title('Confusion Matrix')
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.show()

#Logistic Regression
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.utils import resample

# Data Cleaning (optional)
data_cleaned = data.drop_duplicates().dropna()


# Identify categorical columns
categorical_cols = X.select_dtypes(include=['object']).columns

# One-hot encode categorical columns
X_categorical = pd.get_dummies(X[categorical_cols])

# Concatenate one-hot encoded columns with original numeric columns
X_processed = pd.concat([X.drop(columns=categorical_cols), X_categorical], axis=1)

# Imputation
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X_processed)

# Normalization/Standardization
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed)

# Balancing Classes
X_balanced, y_balanced = resample(X_scaled, y, replace=True, n_samples=X_scaled.shape[0])

# Splitting into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)

# Model Training
model = LogisticRegression()
model.fit(X_train, y_train)

# Model Evaluation
y_pred = model.predict(X_test)

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(conf_matrix)

# Accuracy Score
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
# Plot Confusion Matrix as Heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')
plt.title('Confusion Matrix')
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.show()

#svm
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.utils import resample

# Identify categorical columns
categorical_cols = X.select_dtypes(include=['object']).columns

# One-hot encode categorical columns
X_categorical = pd.get_dummies(X[categorical_cols])

# Concatenate one-hot encoded columns with original numeric columns
X_processed = pd.concat([X.drop(columns=categorical_cols), X_categorical], axis=1)

# Imputation
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X_processed)

# Normalization/Standardization
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed)

# Balancing Classes
X_balanced, y_balanced = resample(X_scaled, y, replace=True, n_samples=X_scaled.shape[0])

# Splitting into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)

# Model Training
model = SVC()
model.fit(X_train, y_train)

# Model Evaluation
y_pred = model.predict(X_test)

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(conf_matrix)

# Accuracy Score
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Plot Confusion Matrix as Heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')
plt.title('Confusion Matrix')
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.show()

#KNN
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.utils import resample


# Balancing Classes
X_balanced, y_balanced = resample(X_scaled, y, replace=True, n_samples=X_scaled.shape[0])

# Splitting into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)

# Model Training
model = KNeighborsClassifier()
model.fit(X_train, y_train)

# Model Evaluation
y_pred = model.predict(X_test)

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(conf_matrix)

# Accuracy Score
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
# Plot Confusion Matrix as Heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')
plt.title('Confusion Matrix')
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.show()

#Decision Tree
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.utils import resample

# Identify categorical columns
categorical_cols = X.select_dtypes(include=['object']).columns

# One-hot encode categorical columns
X_categorical = pd.get_dummies(X[categorical_cols])

# Concatenate one-hot encoded columns with original numeric columns
X_processed = pd.concat([X.drop(columns=categorical_cols), X_categorical], axis=1)

# Imputation
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X_processed)

# Normalization/Standardization
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed)

# Balancing Classes
X_balanced, y_balanced = resample(X_scaled, y, replace=True, n_samples=X_scaled.shape[0])

# Splitting into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)

# Model Training
model = DecisionTreeClassifier()
model.fit(X_train, y_train)

# Model Evaluation
y_pred = model.predict(X_test)

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(conf_matrix)

# Accuracy Score
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
# Plot Confusion Matrix as Heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')
plt.title('Confusion Matrix')
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.show()

#XGB Classifier
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from xgboost import XGBClassifier
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.utils import resample
import matplotlib.pyplot as plt
import seaborn as sns


# Identify categorical columns
categorical_cols = X.select_dtypes(include=['object']).columns

# One-hot encode categorical columns
X_categorical = pd.get_dummies(X[categorical_cols])

# Concatenate one-hot encoded columns with original numeric columns
X_processed = pd.concat([X.drop(columns=categorical_cols), X_categorical], axis=1)

# Imputation
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X_processed)

# Normalization/Standardization
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed)

# Balancing Classes
X_balanced, y_balanced = resample(X_scaled, y, replace=True, n_samples=X_scaled.shape[0])

# Splitting into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)

# Model Training
model = XGBClassifier()
model.fit(X_train, y_train)

# Model Evaluation
y_pred = model.predict(X_test)

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(conf_matrix)

# Accuracy Score
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Plot Confusion Matrix in a Box
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted labels")
plt.ylabel("True labels")
plt.show()

#ANN
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.utils import resample
import matplotlib.pyplot as plt
import seaborn as sns

# Identify categorical columns
categorical_cols = X.select_dtypes(include=['object']).columns

# One-hot encode categorical columns
X_categorical = pd.get_dummies(X[categorical_cols])

# Concatenate one-hot encoded columns with original numeric columns
X_processed = pd.concat([X.drop(columns=categorical_cols), X_categorical], axis=1)

# Imputation
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X_processed)

# Normalization/Standardization
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed)

# Balancing Classes
X_balanced, y_balanced = resample(X_scaled, y, replace=True, n_samples=X_scaled.shape[0])

# Splitting into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)

# Model Training
model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, activation='relu', solver='adam', random_state=42)
model.fit(X_train, y_train)

# Model Evaluation
y_pred = model.predict(X_test)

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(conf_matrix)

# Accuracy Score
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Plot Confusion Matrix in a Box
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted labels")
plt.ylabel("True labels")
plt.show()

import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Load dataset
iris = load_iris()
X, y = iris.data, iris.target

# Data preprocessing
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Define Ant Colony Optimization for feature selection
class AntColonyOptimization:
    def __init__(self, n_ants, n_iterations):
        self.n_ants = n_ants
        self.n_iterations = n_iterations

    def select_features(self, X_train, y_train):
        n_features = X_train.shape[1]
        pheromones = np.ones(n_features)  # Initialize pheromone levels

        for _ in range(self.n_iterations):
            ant_paths = []
            for _ in range(self.n_ants):
                # Construct ant's solution (selected features)
                selected_features = np.random.choice([0, 1], size=n_features, p=[0.5, 0.5])
                ant_paths.append(selected_features)

            # Update pheromone levels based on ant's solutions
            pheromones *= 0.5  # Evaporation
            for path in ant_paths:
                pheromones += path

        # Select features based on pheromone levels
        selected_features = (pheromones > np.median(pheromones)).astype(int)
        return selected_features

# Define SVM classifier
svm = SVC(kernel='linear')

# Define function for accuracy and confusion matrix
def evaluate_performance(classifier, X_train, X_test, y_train, y_test):
    classifier.fit(X_train, y_train)
    y_pred = classifier.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)
    return acc, cm

# Hyperparameters
n_ants = 10
n_iterations = 50

# Run ACO for feature selection
aco = AntColonyOptimization(n_ants=n_ants, n_iterations=n_iterations)
selected_features = aco.select_features(X_train, y_train)

# Apply selected features
X_train_selected = X_train[:, selected_features.astype(bool)]
X_test_selected = X_test[:, selected_features.astype(bool)]

# Evaluate performance
accuracy, confusion_matrix = evaluate_performance(svm, X_train_selected, X_test_selected, y_train, y_test)

# Print results
print("Selected features:", selected_features)
print("Accuracy:", accuracy)
print("Confusion matrix:\n", confusion_matrix)

# Plot confusion matrix
sns.set(font_scale=1.2)
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_matrix[1:, 1:], annot=True, fmt="d", cmap="Blues", cbar=False, square=True,
            xticklabels=iris.target_names[1:], yticklabels=iris.target_names[1:])
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

import matplotlib.pyplot as plt

# Model names
models = ['Random Forest', 'Linear Regression', 'SVM', 'KNN', 'Decision Tree', 'XGB', 'ANN', 'Ant Colony']

# Accuracy scores
accuracy_scores = [80, 78, 77, 60, 81, 74, 75, 93]

# Plotting the graph
plt.figure(figsize=(10, 6))
plt.bar(models, accuracy_scores, color='skyblue')
plt.xlabel('Machine Learning Models')
plt.ylabel('Accuracy (%)')
plt.title('Accuracy of Different Machine Learning Models')
plt.ylim(0, 100)  # Set y-axis limit from 0 to 100
plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add gridlines
plt.show()